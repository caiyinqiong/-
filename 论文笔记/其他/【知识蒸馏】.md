![image-20210120151530224](../../images/image-20210120151530224.png)

##### Logits(Response)-based knowledge：

- Distilling the Knowledge in a Neural Network Hilton NIPS 2014

  ![image-20210120152630381](../../images/image-20210120152630381.png)

- Deep mutual learning CVPR 2018

  ![image-20210120153344109](../../images/image-20210120153344109.png)

- On the efficacy of knowledge distillation, ICCV 2019

  ![image-20210120154336960](../../images/image-20210120154336960.png)

- Self-training with noisy student improves imagenet classification 2019

  ![image-20210120154539077](../../images/image-20210120154539077.png)

- Training deep neural networks in generations: A more tolerant teacher educates better students AAAI 2019

- Distillation-based training for multi-exit architectures ICCV 2019

  ![image-20210120155458910](../../images/image-20210120155458910.png)

##### Feature-based：

- Fitnets: Hints for thin deep nets. ICLR 2015
- Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. ICLR 2017

##### Relation-based：

- A gift from knowledge distillation: Fast optimization, network minimization and transfer learning CVPR 2017
- Similarity-preserving knowledge distillation ICCV 2019