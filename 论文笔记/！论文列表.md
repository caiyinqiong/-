其他

- [x] Rethinking Query Expansion for BERT Reranking（ECIR2020）

  > BERT reranker更适合于自然语言句子的长query，所以传统模型的问题扩展方法可能并不太适用于BERT-based reranker。本文探究了新的问题扩展方法，发现结合扩展【结构词】和【概念词】的方法最有效。

- [x] 【Transformer-kernel】Interpretable & Time-Budget-Constrained Contextualization for Re-Ranking（ECAI2020）

  > 网络结构：对query和document分别采用三层Transformer，对contextual word embedding结果计算交互矩阵，再进行kernel-pooling。
  > 卖点：1）相比BERT来说效率更高，相比非BERT的模型性能更好，所以达到effective和efficiency的平衡。2）采用交互矩阵和kernel-pooling，具有更好的解释性，因为可以看到document每个term的得分。
  > 实验数据集：MARCO-passge、MARCO-document、TREC-CAR2017
  > 亮点：评估效率的方式，比较了在相同的时间限制下，每个模型的MRR、NDCG、Recall性能。而不是像其他工作中仅比较处理每个document/query所需的时间。
  
- [x] 【WMD：word mover‘s distance】From Word Embeddings To Document Distances（2015，ICML）

  > 定义一种Word mover distance，由word embedding之间的距离得到docuemnt之间的距离。主要用于文本分类任务，原论文中不涉及检索任务。
  
- [x] Reqa: an evaluation for end-to-end answer retrieval models（2019）

  > 本文把SQUAD和NQ数据集改编成ReQA任务，即introduce Retrieval QuestionAnswering (ReQA) as a new benchmark for evaluating end-to-end answer retrieval models.
  >
  > ReQA任务：The task assesses how well models are able to retrieve relevant sentence-level answers to queries from a large corpus.

- [x] PARADE: Passage representation aggregation for document reranking

  > 提出了三种由passage-level representation得到document-level representation，进而得到document打分的方法：PARADE_Max、PARADE_Attn、PARADE_Transformer。
  
- [ ] Learning a Beer Negative Sampling Policy with Deep Neural Networks for Search（ICTIR2019）

  > we establish that using reinforcement learning to optimize a policy over a set of sampling functions.
  
- [x] BERT-QE: Contextualized Query Expansion for Document Re-ranking

  > this paper proposes a novel query expansion model that leverages the strength of the BERT model to select relevant document chunks for expansion



# 2020

- [x] Neural Passage Retrieval with Improved Negative Contrast

  > We explore four negative sampling strategies: 1) coarse semantic similarity, 2) fine semantic similarity, 3) BM25 negative, 4) context negative.
  >
  > We train the dual encoder models in two stages: pre-training with synthetic data and ﬁne tuning with domain-speciﬁc data. We apply negative sampling to both stages.
  >
  > it is not evident that there is one single sampling strategy that works best in all the tasks. mixing the negatives from different strategies achieve performance on par with the best performing strategy in all tasks.

- [x] Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation

  > we explore a supervised data augmentation approach leveraging a complex classiﬁcation model with cross-attention between questionanswer pairs.

- [ ] 

# 2021

- [x] Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline（ECIR）

  > 随着深度语言模型在第一阶段检索中的应用（如doc2query，DeepCT，HDCT），使得第一阶段检索得到的结果有更多的hard negative。之前用于reranking阶段的BERT模型通常使用binary cross entropy目标函数，但在强retriever的背景下，这不再是最优的模型训练方式。对此，本文提出了Localized Contrastive Estimation目标函数。

- [x] Optimizing Dense Retrieval Model Training with Hard Negatives

  > 提出了两种dense retrieval model训练策略：
  >
  > 1）STAR：static hard negative samling方法的训练不太稳定。为了稳定训练过程，引入random negatives。而且为了在引入random negatives的同时不增加计算代价，采用in-batch策略。
  >
  > 2）ADORE：训练过程固定doc embedding，只训练query embedding。从而可以时时得到真正的dynamic hard negatives。

- [x] OpenMatch: An Open-Source Package for Information Retrieval

  > 清华开源的NeuIR工具包。
  >
  > <img src="../images/image-20210310150321343.png" alt="image-20210310150321343" style="zoom:33%;" />

- [x] Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation

  > Following our observation that different architectures converge to different scoring ranges, we proposed to optimize not the raw scores, but rather the margin between a pair of relevant and non-relevant passages with a Margin-MSE loss.

- [x] Improving Bi-encoder Document Ranking Models with Two Rankers and Multi-teacher Distillation

  > we propose TRMD, a method enabling a bi-encoder model to learn from cross-encoder and bi-encoder teachers by applying multi-teacher knowledge distillation.

- [x] Composite Re-Ranking for Efficient Document Search with BERT

  > With token encoding, BECR effectively approximates the query representations and makes a proper use of deep contextual and lexical matching features, allowing for strong ad-hoc ranking performance to be achieved on TREC datasets.

- [x] TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Eicient Retrieval

- [x] Towards Robust Neural Retrieval Models with Synthetic Pre-Training

  > 本文提出DPR模型（用NQ数据集微调训练的）的zero-shot迁移能力不强，尤其是领域差异较大时。本文提出用合成的数据来先对DPR进行预训练，然后再用NQ数据集进行DPR的微调，这样就能获得较好zero-shot性能。其中合成的数据的指：先用NQ数据集（question+passage）微调BART生成器，然后用训练好的BART生成器对Wikipedia的每个passage生成question，拿生成的question-passage预训练DPQ模型。

- [x] Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling

  > We proposed to improve dense passage retrieval training with a cost-neutral topic aware (query) and balanced margin (passage pairs) sampling strategy, called TAS-Balanced.
  >
  > 而且使用BERT_CAT和ColBERT两个模型进行蒸馏。

- [x] COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List

  > 对dual encoder得到的上下文词嵌入（降维后）进行倒排索引。计算相似度时只看doc中存在的query term与 query term的相似度，再把所有query term的相似度相加。

- [x] UHD-BERT: Bucketed Ultra-High Dimensional Sparse Representations for Full Ranking

  > 在双塔BERT的基础上学习高维稀疏表示。

- [x] Learning Passage Impacts for Inverted Indexes

  > 先用DocT5Query对文档进行扩展，然后对原文档的词和扩展词进行term impact的计算。

- [x] Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval

  > our method mimics the real queries by an iterative K-means clustering algorithm.

- [x] Pre-trained Language Model based Ranking in Baidu Search

  > we perform relevance-oriented pre-training using large-scale user behavioral data and design a tree-based calibration model to refine the noisy and biased clicking data.

- [x] GLOW : Global Weighted Self-Attention Network for Web

  > It learns semantic representation for both queries and documents by integrating global weight into attention score calculation.

- [x] Context-Aware Learning to Rank with Self-Attention

  > In this paper, we propose a context-aware neural network model that learns item scores by applying a self-attention mechanism.

- [x] Distilling Knowledge from Reader to Retriever for Question Answering

  > reader中的attention分布作为retriever的监督信号。

- [x] End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering

  > openQA的retriever和reader联合训练。

- [x] Efﬁcient Passage Retrieval with Hashing for Open-domain Question Answering

- [x] Whitening Sentence Representations for Better Semantics and Faster Retrieval

  > the whitening operation in traditional machine learning can similarly enhance the isotropy of sentence representations and achieve competitive results.

- [x] SimCSE: Simple Contrastive Learning of Sentence Embeddings

  > We present an unsupervised approach which predicts input sentence itself with dropout noise and a supervised approach utilizing NLI datasets.
  
- [x] Learnt Sparsity for Effective and Interpretable Document Ranking

  > 对document ranking，先选句子，再计算相关性。

- [x] Pre-trained Language Model for Web-scale Retrieval in Baidu Search

  > The system employs 1) an ERNIE-based retrieval model, 2) a multi-stage training paradigm and 3) a unified workflow for the retrieval system.

- [x] Reader-Guided Passage Reranking for Open-Domain Question Answering

  >we propose a simple and effective passage reranking method, named ReaderguIDEd Reranker (R IDER), which does not involve training and reranks the retrieved passages solely based on the top predictions of the reader before reranking.

- [ ] 






# （完成）SIGIR2018

- [x] （IR匹配）Modeling Diverse Relevance Patterns in Ad-hoc Retrieval（范意兴师兄）

- [x] ！！（排序）Learning a Deep Listwise Context Model for Ranking Refinement

- [x] ！！（点击模型）A Click Sequence Model for Web Search

- [x] ！！（多跳问答匹配）Multihop Attention Networks for Question Answer Matching

- [x] ！！（对话系统的response排序）Response Ranking with Deep Matching Networks and External Knowledge in Information-seeking Conversation Systems

- [x] ！！（句子相似度建模、对抗学习）CAN: Enhancing Sentence Similarity Modeling with Collaborative and Adversarial Network

- [x] ！！（CQA的相似问题识别）Related or Duplicate: Distinguishing Similar CQA Questions via Convolutional Neural Networks

- [x] ！！（QA排序、知识库）Knowledge-aware Attentive Neural Network for Ranking Question Answer Pairs

- [x] ！！（融合知识库）Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling

- [x] （TREC-CAR任务，对question path进行分段处理）Characterizing Question Facets for Complex Answer Retrieval

- [x] （无监督的QA任务的baseline）Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering

- [x] ！！（IR）Multi-level Abstraction Convolutional Model with Weak Supervision for Information Retrieval

- [ ] ！！（点击相关性数据集）**Sogou-QCL: A New Dataset with Click Relevance Label**

- [ ] ！！（QA数据集）**WikiPassageQA: A Benchmark Collection for Research on Non-factoid Answer Passage Retrieval**

- [x] ！！（对话和用户意图分析的数据集）Analyzing and Characterizing User Intent in Information-seeking Conversations

  

- [ ] ！！（文本生成平台）Texygen: A Benchmarking Platform for Text Generation Models

- [ ] XXX（IR）Modeling multidimensional user relevance in IR using vector spaces

- [ ]  XXX（IR）Are we on the Right Track? Integrating Theoretical and Empirical Methodologies for Information Retrieval

- [ ]  XXX（概率信息检索模型）A New Term Frequency Normalization Model for Probabilistic Information Retrieval

- [ ] XXX（L2R）Universal Approximation Functions for Fast Learning to Rank

- [ ] XXX (搜索的点击模型) Constructing Click Models for Mobile Search

- [ ] XXX（电商搜索中的问题分类）A taxonomy of queries for e-commerce search

- [ ] XXX（问答）Characterizing and Supporting Question Answering in Human-to-Human Communication

- [ ] XXX (搜索结果的多样性) From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks （徐君老师）

- [ ] Predicting User Knowledge Gain in Informational Search Sessions

- [ ] （排序的鲁棒性）Ranking Robustness under Adversarial Document Manipulations

- [ ] ？？（一个信息检索框架）An Information Retrieval Framework for Contextual Suggestion Based on Heterogeneous Information Network Embeddings

- [ ] ？？Deep Semantic Text Hashing with Weak Supervision

- [ ] ？？（排序中的公平性）Equity of Attention: Amortizing Individual Fairness in Rankings

- [ ] ？？Generating Better Queries for Systematic Reviews

- [ ] ？？Neural Compatibility Modeling with Attentive Knowledge Distillation

- [ ] ？？Measuring the Utility of Search Engine Result Pages

- [ ] ？？Ranking Documents by Answer-Passage Quality

- [ ] ？？（相关性的理论）What Can Rationales behind Relevance Judgments Tell Us About Assessor Disagreement?

- [ ] ？？（相关性的理论）Testing the Cluster Hypothesis with Focused and Graded Relevance Judgment

- [ ] ？？Neural Query Performance Prediction using Weak Supervision from Multiple Signals

- [ ] ？？Query Performance Prediction using Passage Information

- [ ] ？？Query Performance Prediction Focused on Summarized Letor Features

- [ ] ？？Query Performance Prediction and Effectiveness Evaluation Without Relevance Judgments: Two Sides of the Same Coin

- [ ] ？？Query Variation Performance Prediction for Systematic Reviews

- [ ] ？？Efficient Exploration of Gradient Space for Online Learning to Rank

- [ ] ？？Selective Gradient Boosting for Effective Learning to Rank

- [ ] ？？Item Retrieval as Utility Estimation

- [ ] ？？Attention-based Hierarchical Neural Query Suggestion

- [ ] ？？Optimizing Query Evaluations using Reinforcement Learning for Web Search

- [ ] （network embedding）BiNE: Bipartite Network Embedding

- [ ] Translating Representations of Knowledge Graphs with Neighbors





# （完成）SIGIR2019

- [x] （CQA中的重复问题检测）Adaptive Multi-Attention Network Incorporating Answer Information for Duplicate Question Detection

  > 动机：paired answers can provide effective information for duplicate question detection while they may simultaneously introduce noise to the detection.
  >
  > 方法：We propose an adaptive multi-attention network (AMAN), an effective method integrating external knowledge from answers for duplicate identification and filtering out the noise introduced by paired answers adaptively.

- [x] Controlling Risk of Web Question Answering

- [ ] （人类行为）Human Behavior Inspired Machine Reading Comprehension

- [x] （人类行为）Teach Machine How to Read: Reading Behavior Inspired Relevance Estimation

  > 由人类阅读的6条启发式规则设计检索模型。

- [x] （人类行为）Investigating Passage-level Relevance and Its Role in Document-level Relevance Judgment

  > 如何由passage-level的相关性得到document-level的相关性。

- [x] CEDR: Contextualized Embeddings for Document Ranking

  > 把BERT得到的contextual embedding 融合到现有的neural ranking models（KNRM、DRMM、PACRR）

- [x] Content-Based Weak Supervision for Ad-Hoc Re-Ranking

  > We presented an approach for employing content-based sources of pseudo relevance for training neural IR models.
  >
  > We also showed that performance can be boosted using two ﬁltering techniques: one heuristic-based and one that re-purposes a neural ranker

- [x] Deeper Text Understanding for IR with Contextual Neural Language Modeling

  > 用bing search log数据微调Bert，再在Robust和CluWeb上进行rerank BoW top-100的实验。
  >
  > 使用Bert时把document切分成passage，再使用BERT-FirstP、BERT-MaxP、BERT-SumP三种组合方式。

- [x] FAQ Retrieval Using Attentive Matching

  > 本文提出了多种方法解决FAQ检索。We compared various possible aggregation methods to e￿ectively represent query, question and answer information, and observed that answers in FAQs can provide valuable and bene￿cial information for retrieval models, if properly aggregated.
  > 表明了在FAQ检索中也可以使用answer的信息。

- [x] FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance

  > This paper presented a method for using TSUBAKI-based query-question similarity and BERT-based query-answer relevance in a FAQ retrieval task.

- [x] History Modeling for Conversational Question Answering

  > 提出了一个模型 for 对话式QA。

- [x] Learning More From Less: Towards Strengthening Weak Supervision for Ad-Hoc Retrieval

  > presented two approaches(noise-aware model 和 influence-aware model) to reduce the amount of weak data needed to surpass the performance of the unsupervised method that generates the training data.

- [x] 【MMN】Multi-level Matching Networks for Text Matching

  > 动机：A major limitation of existing works is that only high level contextualized word representations are utilized to obtain word level matching results without considering other levels of word representations, thus resulting in incorrect matching decisions for cases where two words with different meanings are very close in high level contextualized word representation space.
  >
  > 方法：instead of making decisions utilizing single level word representations, a multi-level matching network (MMN) is proposed in this paper for text matching, which utilizes multiple levels of word representations to obtain multiple word level matching results for final text level matching decision.

- [x] Document Gated Reader for Open Domain Question Answering（计算每个文档与query的相似度时考虑其他文档的信息；将文档的相似度与answer span的概率相乘，在全局进行normalize；用bootstrapping数据生成机制解决远程监督带来的假正例问题）

- [x] An axiomatic approach to regularizing neural ranking models

  > 使用一些公理对document进行扰动，使得扰动后的文本比原文本更相关或者更不相关，从而得到更多的监督信号，帮助模型的训练。

- [ ] 



# （完成）SIGIR2020

- [x] （TKL）Local Self-Attention over Long Text for Efficient Document Retrieval

  > 对Transformer-kernel的改进，主要是针对Transformer-kernel用于长文本时。之前的基于Transformer的模型用于长文本时通常的做法是直接截断。本文在Transformer-kernel模型的基础上提出了local self-attention。并且计算出每个区域的相关度之后再使用top-local-max机制组合得到全局相关度。
  > 数据集：MARCO的document retrieval数据集

- [x] **Training Curricula for Open Domain Answer Re-Ranking**

  > for passage re-ranking 阶段。
  >
  > 提出课程学习策略，改进BERT和Conv-KNRM的训练过程，刚开始给容易的passage更大的权重，给难的passage更小的权重，后期给相同的权重。

- [x] ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT

- [x] 【PreTTR】Efficient Document Re-Ranking for Transformers by Precomputing Term Representations

- [ ] SetRank: Learning a Permutation-Invariant Ranking Model for Information Retrieval

- [x] Open-Retrieval Conversational Question Answering

  > 提出了一个开放域对话式QA数据集：OR-QuAC。
  >
  > 提出了一个baseline，包括retriever、re-ranker，reader。

- [x] Match$^2$: A Matching over Matching Model for Similar Question Identification

- [x] **MarkedBERT: Integrating Traditional IR cues in pre-trained language models for passage retrieval**

  > We proposed MarkedBERT that incorporates Exact Matching signals via a simple yet effective marking technique that only modifies the model input. 

- [x] Context-Aware Term Weighting For First-Stage Passage Retrieval

- [x] Learning Term Discrimination

- [x] Improving Contextual Language Models for Response Retrieval in Multi-Turn Conversation

  > 对BERT类的上下文预训练模型提出针对多轮对话任务的两种改进策略：1）Speaker Segmentation；2）Dialogue Augmentation

- [x] Large-scale Image Retrieval with Sparse Binary Projections

- [x] 【EPIC】**Expansion via Prediction of Importance with Contextualization**

  > 把query表示成稀疏向量，document表示成dense向量，计算点积。

- [x] Efficiency Implications of Term Re-Weighting for Passage Retrieval

- [x] DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding

- [x] Improving Matching Models with Hierarchical Contextualized Representations for Multi-turn Response Selection

- [x] **Distilling Knowledge for fast retrieval-based chat-bots**

  > we introduced an enhanced BERT cross-encoder architecture modiﬁed for the task of response retrieval. Alongside that, we utilized knowledge distillation to compress the complex BERT cross-encoder network as a teacher model into the student BERT bi-encoder model. This increases the BERT bi-encoders prediction quality without aﬀecting its inference speed.

- [x] **Having Your Cake and Eating it Too: Training Neural Retrieval for Language Inference without Losing Lexical Match**

  > We presented a simple approach to infuse lexical matching using unsupervised IR methods into a state-of-the-art transformer method - RoBERTa. We show that infusing lexical-matching improves the performance on simpler retrieval based question and the (justification) retrieval task itself。

- [x] An analysis of BERT in document ranking

  > What Does BERT Look At? An Analysis of BERT’s Attention.
  >
  > How Contextual are Contextualized Word Representations.
  >
  > Understanding the Behaviors of BERT in Ranking（刘知远）

- [x] Read, Attend, and Exclude:  Multi-Choice Reading Comprehension by Mimicking Human Reasoning Process

  > 提出了一个针对多选型阅读理解任务的模型。

- [x] Unsupervised Text Summarization with Sentence Graph Compression

  > 对多篇文档生成摘要。



# SIGIR2021

- [ ] ~~Optimizing Training of Pre-trained Rerankers in Multi-Stage Dense Retrieval~~

  > 提出【动态多粒度】的训练方法，解决不充分的正例标注问题。

- [x] ~~Dense Representation for Few-sample Knowledge Retrieval~~

  > 没有足够的数据对训练dense retriever的问题。

- [ ] ~~On Single and Multiple Representations in Dense Passage Retrieval~~

  > 比较了一些单表达（ANCE）和多表达模型（ColBERT），发现对于难的query，多表达模型更有用。

- [x] Learning Robust Dense Retrieval Models from Incomplete Relevance Labels

  > 解决负样本标注不充分的问题，使用具有完整相关性标注的开发集（e.g., TREC DL）来评估负样本分布，从而帮助dense retrieval模型的训练。

- [ ] ~~Analysing Dense Passage Retrieval for Multi-Hop Questions~~

  > 分析dense retrieval模型在多跳问答上成功的原因。

- [x] SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking

  > 在SparTerm的基础上，提出explicit sparsity regularization and a log-saturation effect on term weights。
  
- [ ] ~~Not All Relevance Scores are Equal: Efficient Uncertainty and Calibration Modeling for Deep Retrieval Models~~

- [x] Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking

  > we adopt an intra-document cascading strategy, which prunes passages of a candidate document using a less expensive model, called ESM, before running a scoring model that is more expensive and effective, called ETM. We found it best to train ESM (short for Efficient Student Model) via knowledge distillation from the ETM (short for Effective Teacher Model) e.g., BERT.

- [x] Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling

- [x] B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval

- [x] Few-Shot Conversational Dense Retrieval

- [x] Optimizing Dense Retrieval Model Training with Hard Negatives

- [x] Binary Neural Network Hashing for Image Retrieval

  > we propose a novel deep hashing method, called Binary Neural Network Hashing (BNNH) for fast image retrieval.

- [ ] Answer Complex Questions: Path Ranker Is All You Need

- [ ] IDRQA: Iterative Document Reranking for Open-domain Multi-hop Question Answering

- [ ] TILDE: Term Independent Likelihood moDEl for Passage Re-ranking

- [ ] Learning Early Exit Strategies for Additive Ranking Ensembles

- [x] Contextualized Offline Relevance Weighting for Efficient and Effective Neural Retrieval

  > the neighbour documents of selected seed documents are quickly re-ranked using pre-computed relevance and pseudo-queries.

- [x] Improving Bi-encoder Document Ranking Models with Two Rankers and Multi-teacher Distillation

- [ ] Text-to-Text Multi-view Learning for Passage Re-ranking

- [ ] Composite Code Sparse Autoencoders for First Stage Retrieval

- [ ] KeyBLD: Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval

- [ ] Significant Improvements over the State of the Art? A Case Study of the MS MARCO Document Ranking Leaderboard

- [x] Learning Passage Impacts for Inverted Indexes

- [ ] Improving Transformer-Kernel Ranking Model Using Conformer and Query Term Independence

- [ ] A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models

- [x] Joint Learning of Deep Retrieval Model and Product Quantization based Embedding Index

- [x] Passage Retrieval for Outside-Knowledge Visual Question Answering

- [x] MS MARCO: Benchmarking Ranking Models in the Large-Data Regime

- [x] 【CBNS】Cross-Batch Negative Sampling for Training Two-Tower Recommenders

  > analyse the embedding stability，提出cross-batch负采样策略。





# ACL2018

- [x] Entity-duet neural ranking: Understanding the role of knowledge graph semantics in neural information retrieval



# （完成）ACL2019

- [x] RankQA: Neural Question Answering with Answer Re-Ranking 

- [x] Episodic Memory Reader: Learning What to Remember for Question Answering from Streaming Data 

- [x] Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension

- [x] Latent Retrieval for Weakly Supervised Open Domain Question Answering

- [x] Multi-Hop Paragraph Retrieval for Open-Domain Question Answering

- [x] Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index

- [x] A cross-sentence latent variable model for semi-supervised sequence matching

- [x] Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction 

- [x] Learning a Matching Model with Co-teaching for Multi-turn Response Selection in Retrieval-based Dialogue Systems

- [x] Matching Article Pairs with Graphical Decomposition and Convolutions

- [x] RE2: Simple and Effective Text Matching with Richer Alignment Features

- [x] A Lightweight Recurrent Network for Sequence Modeling

- [x] Encouraging Paragraph Embeddings to Remember Sentence Identity Improves Classification

- [x] Pretraining Methods for Dialog Context Representation Learning

- [x] Relational Word Embeddings

- [x] Training Neural Response Selection for Task-Oriented Dialogue Systems

- [x] Learning Transferable Feature Representations Using Neural Networks

- [x] Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue Systems

  > In this paper, instead of conﬁguring new architectures, we investigate how to improve the performance of existing matching models with a better learning method. we consider four negative sampling strategies, namely minimum sampling, maximum sampling, semi-hard sampling, and decay-hard sampling.
  >
  > In the ﬁrst two strategies, a response candidate that corresponds to the minimal or the maximal matching score at the current step is picked from a pool as a negative example for the next step; and in the latter two strategies, we select negative examples by considering how hard they are to the current matching models. The semi-hard sampling prefers candidates with moderate difﬁculty to avoid both false negatives and trivial true negatives, and the decay-hard sampling gradually increases the difﬁculty of negative samples with the training process going on.




第一次阅读列表

- [x] ！！（多段落MRC）**Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs**

- [x] ！！（生成式MRC）**Multi-style Generative Reading Comprehension** 【基于transformer】

- [x] **（NLI数据集中的bias问题）Don’t Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference 【给定假设和label，预测前提。。。方法新颖，但没看懂帖子】**

- [ ] （MRC、推理）Inferential Machine Comprehension: Answering Questions by Recursively Deducing the Evidence Chain from Text 【模型很复杂，，主要针对多步推理】

  


第二次论文列表（20篇）

- [x] ！！（MRC，外部知识）**Explicit Utilization of General Knowledge in Machine Reading Comprehension**【提出一种使用外部知识WordNet的方式】
- [x] ！！（MRC）**Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension**【提出词级别的动态self-attention机制用于多段落MRC】
- [x] ！！（多任务学习）**Multi-Task Deep Neural Networks for Natural Language Understanding** 【在BERT的基础上对GLUE的所有任务联合学习】
- [x] （无监督MRC数据集生成）Unsupervised Question Answering by Cloze Translation
- [x] ！！（BERT+知识图谱）ERNIE: Enhanced Language Representation with Informative Entities
- [x] ！！（问题生成）**Learning to Ask Unanswerable Questions for Machine Reading Comprehension** 【论坛上的帖子没太看懂具体方法，但感觉有价值】
- [x] ！！（QA对生成）**Generating Question-Answer Hierarchies** 【论坛上的帖子没太看懂具体方法，但感觉有价值】
- [x] ！！（MRC）**Exploiting Explicit Paths for Multi-hop Reading Comprehension**
- [x] ！！（self-attention）**Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned**
- [x] ！！（对话）**One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues**  【论坛上的帖子没太看懂具体方法，但感觉有价值】



第三次论文列表

- [x] ！！（QA模型的鲁棒性）**Improving the Robustness of Question Answering Systems to Question Paraphrasing**
- [x] **！！（对话的问题生成）Reinforced Dynamic Reasoning for Conversational Question Generation**
- [x] **Transformer-XL: Attentive Language Models beyond a Fixed-Length Context   **【循环机制 + 相对位置编码】
- [x] ！！！（多跳MRC）**Cognitive Graph for Multi-Hop Reading Comprehension at Scale** 【用两个system解决多跳MRC的推理】
- [x] ！！！（开放域QA）**Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader**【为了弥补KB无法提供开放域QA所需的全部知识，因此考虑加入一些非结构化文本知识】
- [x] ！！（多跳MRC）Multi-hop Reading Comprehension through Question Decomposition and Rescoring
- [x] ！！（MRC）Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension 【在BERT上加入知识库】
- [x] **（对话式QA）MCˆ2: Multi-perspective Convolutional Cube for Conversational Machine Reading Comprehension**
- [x] MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension
- [x] ！！（对话）**Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References**【建模并利用多个valid response之间的关系】
- [x] ！！（对话）**Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading**【融合外部知识，帮助对话系统生成response。。同时提供了一个包括外部知识的新数据集】



- [x] ！！（对话）Improving Multi-turn Dialogue Modelling with Utterance ReWriter

- [x] （对话）Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study

- [x] ！！（对话系统）Reading Turn by Turn: Hierarchical Attention Architecture for Spoken Dialogue Comprehension

- [x] （对话，response生成）Incremental Transformer with Deliberation Decoder for Document Grounded Conversations

- [x] （对话，response选择）Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection

- [x] （对话生成）ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation

- [x] （response 生成）Retrieval-Enhanced Adversarial Training for Neural Response Generation

- [x] （response 生成）Learning to Abstract for Memory-augmented Conversational Response Generation

- [x] （response 生成）Neural Response Generation with Meta-words

- [x] （对话，元学习）Domain Adaptive Dialog Generation via Meta Learning

- [x] ！！（不可回答的问题生成）Self-Attention Architectures for Answer-Agnostic Neural Question Generation

- [x] （图网络、多跳推理）Dynamically Fused Graph Network for Multi-hop Reasoning

- [x] ！！（多步推理）Compositional Questions Do Not Necessitate Multi-hop Reasoning

- [x] （MRC）Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives 

- [x] （MRC）Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension

- [x] ！！（预训练，迁移学习，自然语言生成）Large-Scale Transfer Learning for Natural Language Generation

- [x] Learning Compressed Sentence Representations for On-Device Text Processing

- [x] （BERT可解释性）What Does BERT Learn about the Structure of Language

- [x] Dual Supervised Learning for Natural Language Understanding and Generation

- [x] （梯度反转，域适应）Reversing Gradients in Adversarial Domain Adaptation for Question Deduplication and Textual Entailment Tasks

- [x] （知识蒸馏，多任务学习）BAM! Born-Again Multi-Task Networks for Natural Language Understanding





# （完成）ACL2020

- [x] （效率）DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering（解耦低层的BERT）

- [x] （效率）DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference

  > 本文提出假设：对有些样本而言，不需要通过BERT/RoBERT的所有层就可以得到正确的预测结果，即有些层是冗余的。基于此假设，本文提出的方法是在预训练的BERT/RoBERT的基础上，在每层Transformer后面加一个分类层，然后进行两阶段的参数微调，最后在推断阶段时，如果哪层的分类结果达到置信值，就提前退出。实验部分在GLUE的所有task上进行评估，可以在性能稍降的基础上明显提高效率。

- [x] （效率、蒸馏）FastBERT: a Self-distilling BERT with Adaptive Inference Time

  > 整体模型结构和motivation类似DeeBERT，只是在对每个分类层的训练方法上不太一样，本文使用self-distilling的方式，可以利用无限的unlabel数据，最后来看效果会比DeeBERT更好。实验在6个中文数据集和6个英文数据集上进行，可以在性能几乎不降的情况下3-5倍增加效率。

- [ ] （效率）HAT: Hardware-Aware Transformers for Efficient Natural Language Processing

- [ ] （效率、模型压缩）MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices

- [x] （效率、稀疏表示）Contextualized Sparse Representations for Real-Time Open-Domain Question Answering

  > 对DenSPI的稀疏表示部分的改进，增加可学习的稀疏表示，使用核函数思想。

- [x] A Mixture of h−1 Heads is Better than h Heads

- [x] Highway Transformer: Self-Gating Enhanced Self-Attentive Networks

  > 对Transformer结构进行改进，加入类似highway的思想，从而加快收敛速度，同时取得更好的性能。主要的motivation是：Transformer中multi-head attention的结构只保留了词之间的关系，没有保留词本身的特征。

- [ ] How Does Selective Mechanism Improve Self-Attention Networks?

- [ ] Combining Subword Representations into Word-level Representations in the Transformer Architecture

- [x] （可解释性）Roles and Utilization of Attention Heads in Transformer-based Neural Language Models

  > 1) we suggest an analysis method which helps understand where linguistic properties are learned and represented along attention heads in transformer architectures and 2) we show that using analysis results, attention heads can be maximally utilized for performance gains during the ﬁne-tuning process on the downstream tasks and for capturing linguistic properties.

- [x] （预训练）BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

  > 主要是针对生成式任务（翻译、摘要、QA、对话），同时保证分类任务表现也不差。
  >
  > 论文中也对比了不同预训练目标函数和噪声干扰函数的影响。

- [ ] BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance

- [ ] ？？schuBERT: Optimizing Elements of BERT

- [ ] SenseBERT: Driving Some Sense into BERT

- [ ] （可解释性）Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT

- [x] （预训练）Span Selection Pre-training for Question Answering

- [x] （表示学习）Attentive Pooling with Learnable Norms for Text Representation

  > we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation. we propose two methods to ensure the numerical stability of the model training: The ﬁrst one is scale limiting, which limits the scale of input representations to ensure their non-negativity and avoid potential exponential explosion. The second one is re-formulation, which decomposes the exponent operation into several safe atomic operations to avoid computing the real-valued powers of input features with less computational cost.
  >
  > 评估任务是文本（情感）分类。

- [x] 【MRC】Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension

  > We model documents at different levels of granularity to learn the hierarchical nature of the document. On the Natural Questions dataset, which contains two sub-tasks predicting a paragraph-level long answer and a token-level short answer, our method jointly trains the two sub-tasks to consider the dependencies of the twograined answers.

- [x] （表示学习）Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning

  > we propose a new learning objective named language autoencoding (LAE) for obtaining fully contextualized language representations without repetition. To learn the proposed LAE, we develop a diagonal masking operation and an input isolation mechanism inside the T-TA.

- [x] QUASE: Question-Answer Driven Sentence Encoding

- [x] ReasoningOver Semantic-Level Graph for Fact Checking

  > The main contribution of this work is the graphbased reasoning approach for claim veriﬁcation.

- [x] （表示学习）SPECTER: Document-level Representation Learning using Citation-informed Transformers（利用学术文章之间的引用关系更好的学习学术文章的document表示）

- [x] （课程学习）Curriculum Learning for Natural Language Understanding

  > We explore and demonstrate the effectiveness of CL in the context of ﬁnetuning LM on NLU tasks. We propose a novel CL framework that consists of a Difﬁculty Review method and a Curriculum Arrangement algorithm.

- [x] （问题生成）How to Ask Good Questions? Try to Leverage Paraphrases

- [x] （问题生成）Semantic Graphs for Generating Deep Questions

  > Deep Questions Generation: 生成需要多文本片段推理的复杂问题。
  >
  > we propose a novel framework which ﬁrst constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterwards, we fuse the document-level and graphlevel representations to perform joint training of content selection and question decoding.

- [x] （商品检索）Learning Robust Models for e-Commerce Product Search（通过匹配的query-item pair生成不匹配的query，以便学习更鲁棒的分类界面，更好地识别不匹配的query-item pair。）

- [x] （FAQ）Unsupervised FAQ Retrieval with Question Generation and BERT

  > We presented a fully unsupervised method for FAQ retrieval. The method is based on an initial retrieval of FAQ candidates followed by three rerankers. The ﬁrst one is based on an IR passage retrieval approach, and the others two are independent BERT models that are ﬁne-tuned to predict query-to-answer and query-to-question matching.

- [x] The Cascade Transformer: an Application for Efficient Answer Sentence Selection

  >This work introduces CT, a variant of the traditional transformer model designed to improve inference throughput.  Our approach leverages classiﬁers placed at different encoding stages to prune candidates in a batch and improve model throughput.

- [x] （短文本匹配、图网络）Neural Graph Matching Networks for Chinese Short Text Matching

  > we propose a neural graph matching model for Chinese short text matching.

- [x] （语义匹配）tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection

  > we proposed a ﬂexible framework for combining topic models with BERT.

- [x] （自然语言生成）A Generative Model for Joint Natural Language Understanding and Generation



# ACL2021

- [x] Improving Document Representations by Generating Pseudo Query Embeddings
  for Dense Retrieval

- [x] ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer

  > we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to ﬁne-tune BERT in an unsupervised and effective way.

- [x] PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval

  > we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval: 1) introducing formal formulations of the two kinds of similarity relations, 2) generating high-quality pseudo labeled data via knowledge distillation, 3) designing an effective two-stage training procedure.

- [x] 【APE】Training Adaptive Computation for Open-Domain Question Answering with Computational Constraints

  > by replacing the encoder of generative ODQA models with our proposed adaptive passage encoder, we can train an effective adaptive computation policy without tuning the base model.

- [ ] 

 



# EMNLP2018

- [x] Phrase indexed question answering: A new challenge for scalable document comprehension.（学习独立的document phrase表示和question表示）
- [x] Ranking paragraphs for improving answer recall in open-domain question answering.（对top ranked documents进行段落排序，选择topM个段落进行RC。以便经过retrieval之后可以接触更多的段落，提高answer的召回率）
- [x]  Adaptive Document Retrieval for Deep Question Answering.（探究了retriever和reader之间的关系，对于不同的question，取不同top数目的document用于RC）





# EMNLP2019

- [x] （对话）Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots

- [x] （对话）Multi-Granularity Representations of Dialog

- [x] （文本分类、表示学习）Enhancing Local Feature Extraction with Global Representation for Neural Text Classification

- [x] （表示学习）Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations

- [x] （表示学习）Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

- [x] ~~（句子表示学习）Parameter-free Sentence Embedding via Orthogonal Basis~~

- [x] （机器翻译）Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation

- [x] 【OSOA-DFN】（文本匹配）Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching

- [x] （文本匹配）Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence Matching

  > Given a sentence pair, we extract the output representations of it from BERT. Then we extend BERT with a MatchLSTM layer to get further interaction of the sentence pair for sequence matching tasks.

- [x] （语义匹配）Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling

  > we start with the Pairwise Word Interaction Model, then introduce components for modeling context and structure using multi-layer BiLSTMs and TreeLSTMs.

- [x] 【ADIN】（文本匹配）Asynchronous Deep Interaction Network for Natural Language Inference

  > This model is stacked with multiple inference sub-layers to implement the multi-step reasoning, and each sub-layer consists of two local inference modules in an asymmetrical manner to simulate the asynchronous and interpretable reasoning process.

- [x] 【HCAN】（文本匹配）Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling

  > We propose the HCAN model with a relevance matching module to capture weighted n-gram matching signals and a semantic matching module for contextaware representation learning.
  >
  > experiments show that relevance matching alone performs reasonably well for many NLP tasks, while semantic matching alone is not effective for IR tasks. We show that relevance matching and semantic matching are complementary, and HCAN combines the best of both worlds.

- [x] （QA的采样策略）Improving Answer Selection and Answer Triggering using Hard Negatives

  > We have shown that selection of hard negatives is a powerful tool for answer selection.

- [x] 【GSAMN】（文本匹配）A Gated Self-attention Memory Network for Answer Selection

  > propose a new gated self-attention memory network for Answer selection.

- [x] （文本匹配）MICRON: Multigranular Interaction for Contextualizing RepresentatiON in Non-factoid Question Answering

  > we propose MICRON, allowing to match ﬂexible n-grams and to combine with word-based query term weighting, achieving the state of the art among baselines with reported performances on WikiPassageQA and InsuranceQA.


- [x] （对上下文词向量的分析）How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings

  > 1）In all layers of all three models, the contextualized word representations of all words are not isotropic（即各向异性的）: they are not uniformly distributed with respect to direction. Instead, they are anisotropic, occupying a narrow cone in the vector space.
  >
  > 2）While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-speciﬁc representations.
  >
  > 3）In ELMo, representations of words in the same sentence grow more similar to each other as context-speciﬁcity increases in upper layers; in BERT, they become more dissimilar to each other in upper layers but are still more similar than randomly sampled words are on average; in GPT-2, however, words in the same sentence are no more similar to each other than two randomly chosen words.
  >
  > 4）In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word’s contextualized representations can be explained by a static embedding for that word, providing some justiﬁcation for the success of contextualized representations.

- [x] （对transformer的改进）Tree Transformer: Integrating Tree Structures into Self-Attention

  > This paper proposes Tree Transformer, a ﬁrst attempt of integrating tree structures into Transformer by constraining the attention heads to attend within constituents. The tree structures are automatically induced from the raw texts by our proposed Constituent Attention module.

- [x] （对bert的改进）Fine-tune BERT with Sparse Self-Attention Mechanism

  > we develop a novel Sparse Self-Attention Fine-tuning model (referred as SSAF) which integrates sparsity into selfattention mechanism to enhance the ﬁne-tuning performance of BERT.
  >
  > sparsity is introduced into the self-attention by replacing softmax function with a controllable sparse transformation when ﬁne-tuning with BERT.

- [x] （bert预训练加微调为什么比从0开始训练更好的可解释性）Visualizing and Understanding the Effectiveness of BERT

  > we propose to visualize loss landscapes and optimization trajectories of ﬁne-tuning BERT on speciﬁc datasets.
  >
  > 发现：1）visualization results indicate that BERT pretraining reaches a good initial point across downstream tasks。2）loss landscapes of ﬁne-tuning partially explain the good generalization capability of BERT。3）we demonstrate that the lower (i.e., close to input) layers of BERT are more invariant across tasks than the higher layers。

- [x] 【Birch】（长文本的ad-hoc检索）Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval

  > 用BERT sentence-level的得分得到document-level的得分。
  >
  > Our results demonstrate two surprising ﬁndings: ﬁrst, that relevance models can be transferred quite straightforwardly across domains by BERT, and second, that effective document retrieval requires only “paying attention” to a small number of “top sentences” in each document.

- [x] （外部知识，生成式QA）Incorporating External Knowledge into Machine Reading for Generative Question Answering

- [x] ！！**PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text**

- [x] ！！**Ranking and Sampling in Open-Domain Question Answering**

- [x] ！！Revealing the Importance of Semantic Retrieval for Machine Reading at Scale

- [x] （多跳问答）Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering

  > We introduced ROCC, a simple unsupervised approach for selecting justiﬁcation sentences for question answering, which balances relevance, overlap of selected sentences, and coverage of the question and answer. We coupled this method with a state-of-the-art BERT-based supervised question answering system, and achieved a new state-of-the-art on the MultiRC and ARC datasets among approaches that do not use external resources during training.

- [x] （开放域多跳QA）**Answering Complex Open-domain Questions Through Iterative Query Generation**

  > In the very ﬁrst hop of reasoning, GOLDEN Retriever is presented the original question q, from which it generates a search query q1 that retrieves supporting document d1. Then for each of the subsequent reasoning steps (k = 2, . . . , S), GOLDEN Retriever generates a query q_k from the question and the available context, (q, d_1 , . . . , d_k−1 ). This formulation allows the model to generate queries based on information revealed in the supporting facts.

- [x]  （开放域QA）Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering（提出要跨段落进行global normalization，并使用基于BERT的passage reranker）



# （完成）EMNLP2020

- [ ] *~~AmbigQA: Answering Ambiguous Open-domain Questions~~*

  > 研究开放域问答中问题的歧义性，找到歧义性问题对应的多个答案，并针对每一个答案对问题进行重写以实现消歧。
  >
  > 提出了一个对应的任务和数据集，以及baseline。

- [x] 【**句子表示学习**】*An Unsupervised Sentence Embedding Method by Mutual Information Maximization*

  > <img src="../images/image-20201010154450654.png" alt="image-20201010154450654" style="zoom:33%;" />
  >
  > 句子输入到 BERT 后被编码，其输出的 token embeddings 通过多个不同 kernel size 的一维卷积神经网络得到多个 n-gram 特征。我们把每一个 n-gram 特征当成局部表征（Local representation）， 将平均池化后的局部表征称为全局表征（Global representation）。最后，我们用一个基于互信息的损失函数来学习最终的句向量。该损失函数的出发点是最大化句子的全局表征（句向量）与局部表征之间的平均互信息值，因为对于一个好的全局句向量，它与所对应的局部表征之间的 MI 应该是很高的， 相反，它与其他句子的局部表征间的 MI 应该是很低的。
  >
  > 这样的任务类似 contrastive learning，可以鼓励编码器更好地捕捉句子的局部表征，并且更好地区分不同句子之间的表征。

- [x] 【**模型的可解释性**】*Analyzing Individual Neurons in Pre-trained Language Models*

  > 分析模型：ELMo，T-ELMo，BERT，XLNet
  >
  > 分析方式：从neuron而不是layer的角度进行探测。
  >
  > 一些分析结论：1）it is possible to extract a very small subset of neurons to predict a linguistic task with the same accuracy as using the entire network. Low level tasks such as predicting morphology require fewer neurons compared to high level tasks such as predicting syntax. 2）ELMo generally needed fewer neurons while T-ELMo required more neurons compared to the other models to achieve oracle performance. 说明knowledge of lexical semantics in T-ELMo is distributed in more neurons。3）lexical tasks such as learning morphology (POS tagging) and word semantics (SEM tagging) are dominantly captured by the neurons at lower layers, whereas the more complicated task of modeling syntax (CCG supertagging) is taken care of at the ﬁnal layer. An exception to this overall pattern is the BERT model. Top neurons in BERT spread across all the layers, unlike other models where top neurons (for a particular task) are contributed by fewer layers. 4）BERT is the most distributed model with respect to all properties while XLNet exhibits focus with the most disjoint set of neurons and layers designated for different linguistic properties. 5）Some phenomena (e.g. Verbs) are distributed across many neurons while others (e.g. Interjections) are localized in a fewer neurons。

- [x] *Analyzing Redundancy in Pretrained Transformer Models*

  > 结论：1）相邻层的冗余度高，除了最后两层。2）85%的神经元可以被remove，而且没有性能的损失。大部分冗余的神经元来自于同一层或者相邻层。3）high layer-level redundancy for sequence labeling tasks, The amount of redundancy is substantially lower for sequence classiﬁcation tasks. all the sequence classiﬁcation tasks are learned at higher layers and none of the lower layers were found to be redundant. 4）XLNet is more redundant than BERT。5）Complex core language tasks require more neurons。Less task-speciﬁc redundancy for core linguistic tasks compared to higher-level tasks

- [x] *Context-Aware Answer Extraction in Question Answering**.* （没找到）

- [x] *Dense Passage Retrieval for Open-Domain Question Answering**.*

- [x] *Masking as an Efficient Alternative to Finetuning for Pretrained Language Models**.* 

  > 实验方法：使用预训练模型到下游任务时，不是采用微调所有参数的方法，而是选择一组weights，mask掉其他的参数，最后可以取得和微调差不多的性能。
  > 实验结果：1）Intrinsic evaluations show that masked language models extract valid representations for downstream tasks。2）the representations are generalizable。3）the minima obtained by ﬁnetuning and masking can be easily connected by a line segment。

- [x] Modularized Transfomer-based Ranking Framework.

  > This paper proposes MORES, a modular Transformer ranking framework that decouples ranking into Document Representation, Query Representation, and Interaction.

- [x] *Multi-Step Inference for Reasoning Over Paragraphs*

  > 提出推理模型，分为select+chain+predict三个模块。

- [x] *MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale**.*

  > 结论：1）neither domain similarity nor training data size are suitable for predicting the best models。

- [x] *Multilevel Text Alignment with Cross-Document Attention*

  > 在hierarchical attention 的双塔模型中加入cross-attention。
  >
  > 提出一个doc-doc和sentence-doc对齐的benchmark。

- [x] *On the Sentence Embeddings from BERT for Semantic Textual Similarity**.*

  > BERT-flow

- [ ] ~~*Probing Pretrained Language Models for Lexical Semantics*~~(看不懂)

- [x] *Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting*

  > 在微调阶段使用多任务学习的思想，联合学习预训练任务，但是需要Pretraining Simulation mechanism解决预训练任务不可得的困难，而且多任务学习时要使用Objective Shifting mechanism逐渐偏重下游任务。

- [x] *Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2**.* 

- [ ] *Towards Better Context-aware Lexical Semantics: Adjusting Contextualized Representations through Static Anchors* (找不到)

- [x] FastFormers: Highly Efficient Transformer Models for Natural Language Understanding

  > We showed that utilizing knowledge distillation, structured pruning and numerical optimization can lead to drastic improvements on inference efﬁciency.

- [x] A Little Bit Is Worse Than None: Ranking with Limited Training Data

- [x] Early Exiting BERT for Efficient Document Ranking

  > We propose asymmetric early exiting BERT for document ranking, an effective method to improve model efﬁciency in IR tasks.

- [ ] ~~Don’t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering~~

- [x] Exploring the Boundaries of Low-Resource BERT Distillation

- [ ] ETC: Encoding Long and Structured Inputs in Transformers

  > we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs.
  >
  > The key ideas are a new globallocal attention mechanism, coupled with relative position encodings and a CPC pre-training task.
  
- [x] Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains





# （完成）ICLR2018

（2.3%的口头展示，31.4%的poster接受，9%的workshop）

- [x] ！！！！（DIIN）Natural Language Inference over Interaction Space

- [x] ！！！！（IR，多任务学习）Multi-Task Learning for Document Ranking and Query Suggestion 

- [x] ！！！！（表达学习）An efficient framework for learning sentence representations

- [x] ！！（表达学习，多任务学习）Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning

- [x] ！！！！（序列建模）Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling

- [x] ！！！！（开放域QA）Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering

- [x] ！！！（QA，强化学习）Ask the Right Questions: Active Question Reformulation with Reinforcement Learning

- [x] ！！（MRC）QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension

- [x] ！！（MRC）DCN+: Mixed Objective And Deep Residual Coattention for Question Answering

- [x] ！！（MRC）FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension

  

- [x] （MRC）Multi-Mention Learning for Reading Comprehension with Neural Cascades 

- [x] （摘要）Generating Wikipedia by Summarizing Long Sequences 

- [x] （表达学习）A New Method of Region Embedding for Text Classification 

  

- [ ] （语言模型）breaking_the_softmax_bottleneck：a_high_rank_rnn_language_model

- [ ] （语言模型）Neural Language Modeling by Jointly Learning Syntax and Lexicon

- [ ] （文本生成，GAN）MaskGAN: Better Text Generation via Filling in the _______ 

- [ ] （摘要）A Deep Reinforced Model for Abstractive Summarization

- [ ] （LSTM的可解释性，情感分析）Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs

- [ ] （迁移学习）Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation

- [ ] （图卷积+self-attention）Graph Attention Networks

- [ ] （多任务学习）Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning 

- [ ] All-but-the-Top: Simple and Effective Postprocessing for Word Representations

- [ ] ！！（一个逻辑蕴含的数据集）Can Neural Networks Understand Logical Entailment? 

- [ ] Spherical CNNs

- [ ] （多任务学习）Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering

- [ ] （transformer-based）Non-Autoregressive Neural Machine Translation



# （完成）ICLR2019

- [x] （面向任务型对话系统）Global-to-local Memory Pointer Networks for Task-Oriented Dialogue
- [x] 方法过于数学（对话系统的问题生成）Large-Scale Answerer in Questioner's Mind for Visual Dialog Question Generation
- [x] （对话系统）Wizard of Wikipedia: Knowledge-Powered Conversational Agents
- [x] （NLU）GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding 
- [x] （多文档QA）Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering
- [x] （开放域QA）Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering
- [x] 论文写得很不清晰（QA）Generative Question Answering: Learning to Answer the Whole Question
- [x] （对话式QA）flowQA




- [ ] （对话系统）Detecting Egregious Responses in Neural Sequence-to-sequence Models 
- [ ] （信息检索）textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR

- [ ] Posterior Attention Models for Sequence to Sequence Learning

- [ ] Universal Transformers 
- [ ] Trellis Networks for Sequence Modeling
- [ ] 



# （完成）ICLR2020

第一次阅读列表：

- [x] ！！！**NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension**【提出一个比较好用的 answerability 判定模块，可借鉴到qq匹配任务上】
- [x] ！！！（对话、外部知识）**Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue**【对话任务，如何更好地选择知识】
- [x] **TinyBERT: Distilling BERT for Natural Language Understanding**
- [x] **StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding**
- [x] **ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators**【对bert预训练任务的改进】
- [x] Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension
- [x] （和我的研究好像很像）Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring
- [x] （对BERT的改进）ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
- [x] （对BERT的改进）RoBERTa: A Robustly Optimized BERT Pretraining Approach
- [x] （对话生成，低资源）Low-Resource Knowledge-Grounded Dialogue Generation 【论坛上写的很清晰，方法简明，不用再看了】



第二次阅读列表：

- [x] **！！！！（多跳QA）Transformer-XH: Multi-hop question answering with eXtra Hop attention**
- [x] **！！！！（问题生成）Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation**
- [x] （QA、推理）**Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering**
- [x] ！！！（BERT、机器翻译）**Incorporating BERT into Neural Machine Translation**【不直接使用BERT初始化模型，本文是把BERT的输出融入到SEQ2SEQ模型的每一层，用在每个self-attention之中。此外，本文还提出drop-net——随机丢弃BERT表示或模型本身的表示，充分利用两个方面的信息。实验结果还不错。】
- [x] ！！！（互信息最大化、表达学习）**A Mutual Information Maximization Perspective of Language Representation Learning**



- [x] **（基于知识的预训练语言模型）Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model** 【论坛上写的很清楚】
- [ ] （新的阅读理解数据集）ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning
- [ ] ！！（对transformer的理解）Are Transformers universal approximators of sequence-to-sequence functions?
- [ ] ！！（Transformer的可解释性）Robustness Verification for Transformers 
- [ ] ！！（Transformer）On the Relationship between Self-Attention and Convolutional Layers【好像不少数学 推理】
- [x] **（预训练的检索模型）GOING BEYOND TOKEN-LEVEL PRE-TRAINING FOR EMBEDDING-BASED LARGE-SCALE RETRIEVAL** 【论坛上写的很清楚】
- [ ] （学习word embedding）DeFINE: Deep Factorized Input Word Embeddings for Neural Sequence Modeling
- [ ] （MRC、推理）**Neural Module Networks for Reasoning over Text** 

- [ ] ！！Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models

- [ ] （文本生成、对抗学习）Self-Adversarial Learning with Comparative Discrimination for Text Generation



# （完成）ICLR2021

- [x] Universal Sentence Representations Learning with Conditional Masked Language Model 

  > A novel pre-training technique CMLM for unsupervised sentence representation learning on unlabeled corpora (either in monolingual and multilingual). CMLM通过以相邻句子的编码向量为条件，将句子表示学习整合到MLM训练中。 

- [x] Synthesizer: Rethinking Self-Attention for Transformer Models 

  > This paper seeks to develop a deeper understanding of the role that the dot product self-attention mechanism plays in Transformer models.
  >
  > This paper proposes SYNTHESIZER, a new model that learns to synthesize the self-alignment matrix instead of manually computing pairwise dot products.

- [x] Multi-Head Attention: Collaborate Instead of Concatenate

  > As the multiple heads are inherently solving similar tasks, they can collaborate instead of being independent. Collaborative MHA introduces weight sharing across the key/query projections and decreases the number of parameters and FLOPS.

- [x] Deepening Hidden Representations from Pre-trained Language Models 

  > We argue that only taking single layer’s output restricts the power of pre-trained representation. Thus we deepen the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE), which automatically absorbs the complementary representation with respect to the output from the ﬁnal layer.

- [x] Contextual Knowledge Distillation for Transformer Compression

  > we present a new knowledge distillation strategy for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation. WR is proposed to capture the knowledge of relationships between word representations and LTR deﬁnes how each word representation changes as it passes through the network layers.

- [ ] A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks

- [x] JAKET: Joint Pre-training of Knowledge Graph and Language Understanding

  > These Pre-trained language models models struggle to grasp world knowledge about entities and relations. we propose JAKET, a Joint pre-trAining framework for KnowledgE graph and Text. Under our framework, the knowledge module and language module both provide essential information for each other.

- [x] Optimizing Transformers with Approximate Computing for Faster, Smaller and more Accurate NLP Models 

  > We observe that for a given downstream task, some parts of the pre-trained Transformer are more signiﬁcant to obtain good accuracy, while other parts are less important or unimportant. In order to exploit this observation in a principled manner, we introduce a framework to introduce approximations while ﬁne-tuning a pre-trained Transformer network, optimizing for either size, latency, or accuracy of the ﬁnal network.

- [ ] Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth 

  > In this paper, we study these core questions, through detailed analysis of a family of ResNet models with varying depths and widths trained on CIFAR-10, CIFAR-100 and ImageNet.

- [x] 【MDR】**Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval** 

  > We propose a simple and efﬁcient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER.
  >
  > Our method iteratively encodes the question and previously retrieved documents as a query vector and retrieves the next relevant documents using efﬁcient MIPS methods.
  >
  > It improves DPR by sampling negatives from a memory bank (Wu et al., 2018) — in which the representations of negative candidates are frozen so more candidates can be stored.

- [x] Cluster-Former: Clustering-based Sparse Transformer for Question Answering

  > self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically
  >
  > we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. Cluster-Former consists of two types of encoding layer. The ﬁrst one (noted as SlidingWindow Layer) focuses on extracting local information within a sliding window. The other one (noted as Cluster-Former Layer) learns to encode global information beyond the initial chunked sequences. we stack these two types of layer interchangeably to capture both global and local context efﬁciently.

- [x] Block Skim Transformer for Efficient Question Answering

  > We then propose Block Skim Transformer (BST), a plug-and-play module to the transformer-based models, to accelerate transformer-based models on QA tasks. 
  >
  > By handling the attention weight matrices as feature maps, the CNN-based Block Skim module extracts information from the attention mechanism to make a skim decision. With the predicted block mask, BST skips irrelevant context blocks, which do not enter subsequent layers’ computation.

- [x] Distilling Knowledge from Reader to Retriever for Question Answering

  > In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever.
  >
  > Relevance-guided supervision for openqa with colbert

- [ ] When Do Curricula Work?

- [ ] Rethinking Embedding Coupling in Pre-trained Language Models

- [x] SEED: Self-supervised Distillation For Visual Representation

  > we ﬁnd that existing techniques like contrastive learning do not work well on small networks.
  >
  > Instead of directly learning from unlabeled data, we proposed SEED as a novel self-supervised learning paradigm, which learns representation by self-supervised distillation from a bigger SSL pre-trained model.

- [x] Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval 

  > In this paper, we ﬁrst theoretically show the learning bottleneck of dense retrieval is the domination of uninformative negatives sampled locally in batch, which yield diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. 
  >
  > We then propose Approximate nearest neighbor Negative Contrastive Learning (ANCE), a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index.

- [ ] PMI-Masking: Principled masking of correlated spans

  > 不只是mask一个subtoken，而且mask一个span。

- [ ] Decomposing Mutual Information for Representation Learning

- [ ] Semantic Hashing with Locality Sensitive Embeddings

  > we propose a method for learning continuous representations in which the optimized similarity is the angular similarity.  

- [x] Memory Representation in Transformer 

  > Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model.
  >
  > In this work, we propose and study few extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, (2) creating memory bottleneck for the global information, (3) controlling memory update with dedicated layer.

- [x] Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model

  > In this paper, we argue that randomly sampled masks in MLM would lead to undesirably large gradient variance. To reduce the variance due to the sampling of masks, we proposed a fully-explored masking strategy, where a text sequence is divided into multiple non-overlapping segments. During training, all tokens in one segment are masked out, and the model is asked to predict them with the other segments as the context. It was demonstrated theoretically that the gradients obtained with such a novel masking strategy have a smaller variance, thus enabling more efﬁcient pre-training.
  >
  > 现有的BERT等模型往往采用masked language model进行自监督学习，但是其往往采用随机的方法确定mask的word或者span；本文提出不合适的mask会导致梯度方差变大，并影响模型的效果，并分析原因在于同时mask的word之间具有一定的相似度；故本文提出一种特殊的mask机制，其考虑增大被mask的word之间的差异，进而削弱梯度方差大带来的影响。

- [x] Deep Retrieval: An End-to-End Structure Model for Large-Scale Recommendations

  > we have proposed Deep Retrieval, an end-to-end learnable structure model for largescale recommender systems. DR uses an EM-style algorithm to learn the model parameters and paths of items jointly. Experiments have shown that DR performs well compared with brute-force baselines in two public recommendation datasets.



# 





#ECIR2020 

Patch-based Identification of Lexical Semantic Relations

Relevance Ranking based on Query-Aware Context Analysis

**BERT for Evidence Retrieval and Claim Verification**



# ICTIR2020

**Approximate Nearest Neighbor Search and Lightweight Dense Vector Reranking in Multi-Stage Retrieval Architectures**



# ICTIR2021

- [x] More Robust Dense Retrieval with Contrastive Dual Learning

  > document retrieval loss只能保证document embedding分布的“alignment” and “uniformity”，但query embedding的分布不能保证。本文提出一个dual learning task。

- [x] 【ColBERT-PRF】Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval

  > ColBERT-PRF approach extracts representative feedback embeddings using a clustering technique. It then identifies discriminative embeddings among the representative embeddings and appends them to the query representation.

- [x] A Modern Perspective on Query Likelihood with Deep Generative Retrieval Models

  > We adopt several current neural generative models in our framework and introduce a novel generative ranker (T-PGN), which combines the encoding capacity of Transformers with the Pointer Generator Network model.





# ICML2018

# ICML2019





# IJCAI2018

# IJCAI2019





# NIPS2018

# NIPS2019

# NIPS2020

- [ ] Coresets for Robust Training of Deep Neural Networks against Noisy Labels

- [ ] Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples

- [ ] Part-dependent Label Noise: Towards Instance-dependent Label Noise

- [ ] Exploiting weakly supervised visual patterns to learn from partial annotations

- [ ] Curriculum Learning by Dynamic Instance Hardness

- [ ] Supervised Contrastive Learning

- [ ] Distribution Aligning Refinery of Pseudo-label for Imbalanced Semi-supervised Learning

- [ ] Not All Unlabeled Data are Equal: Learning to Weight Data in Semi-supervised Learning

- [ ] **A Variational Approach for Learning from Positive and Unlabeled Data**

- [ ] **Learning from Positive and Unlabeled Data with Arbitrary Positive Shift**

- [ ] Robust Correction of Sampling Bias using Cumulative Distribution Functions

- [ ] Early-Learning Regularization Prevents Memorization of Noisy Labels

- [ ] BERT Loses Patience: Fast and Robust Inference with Early Exit

- [ ] 【RAG】Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

  > During the training phase, RAG takes an input and encodes it to a high-dimensional vector. This vector then gets used as a query to select the most relevant passages from an external database using Maximum Inner Product Search (MIPS) [Johnson et al., 2017, Guo et al., 2020]. Afterward, The input and the selected set of documents get fed into a generator that produces the ﬁnal answer.
  >
  > All three components are initialized with pre-trained transformers. 

- [ ] **Partial Optimal Transport with applications on Positive-Unlabeled Learning**

- [ ] Big Bird: Transformers for Longer Sequences

- [ ] Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing

- [ ] Pre-training via Paraphrasing

- [ ] DynaBERT: Dynamic BERT with Adaptive Width and Depth

- [ ] Cross-lingual Retrieval for Iterative Self-Supervised Training

- [x] CogLTX: Applying BERT to Long Texts

- [ ] Hard Negative Mixing for Contrastive Learning

- [x] Debiased Contrastive Learning

- [ ] 





# AAAI2018

# AAAI2019

# AAAI2020

- [x] 【NLI、PI】Multi-level Head-wise Match and Aggregation in Transformer for Textual Sequence Matching

  > Multi-level Head-wise Match.

- [x] 【QA】Knowledge and Cross-Pair Pattern Guided Semantic Matching for Question Answering

  > we propose a novel system named KCG for AS, which considers both knowledge and pattern conditions. KCG applies both intra-pair and cross-pair learning. cross-pair learning使用GCN网络，intra-pair learning使用cross attention。最终是cross-pair 和 intra-pair得分的加权和。
  >
  > WikiQA、TrecQA

- [ ] Scalable attentive sentence-pair modeling via distilled sentence embedding

 

# AAAI2021

检索

- [ ] ！！Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval（找不到）

- [ ] ！！Dual Compositional Learning in Interactive Image Retrieval（找不到）

- [x] Learning to Truncate Ranked Lists for Information Retrieval

- [x] HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions

  > In this paper, we propose the HopRetriever to collect reasoning evidence over Wikipedia for multi-hop question answering. Both the structured knowledge indicated by hyperlinks and the unstructured knowledge presented as introductory documents in Wikipedia, are involved and leveraged together in HopRetriever to help the evidence collection.

蒸馏：

- [x] LRC-BERT: Latent-Representation Contrastive Knowledge Distillation for Natural Language Understanding

  > 1）Firstly, the COS-NCE based on contrastive learning is proposed to distill the output of the intermediate layer from the angle distance, which is not considered by the existing knowledge distillation methods. 2）Then, we introduce a gradient perturbation-based training architecture in the training phase to increase the robustness of the model, which is the ﬁrst attempt in knowledge distillation. 3）Finally, to better capture the distribution characteristics of the intermediate layer, we design a two-stage training method for the total distillation loss.

- [x] ALP-KD: Attention-Based Layer Projection for Knowledge Distillation

  > we discussed the importance of distilling from intermediate layers and proposed an attention-based technique to combine teacher layers without skipping them.

- [x] Reinforced Multi-Teacher Selection for Knowledge Distillation

  > We propose a novel RL-based approach, which dynamically assigns weights to teacher models at instance level to better adapt to the strengths of teacher models as well as the capability of student models.

预训练

- [ ] TransTailor: Pruning the Pre-Trained Model for Improved Transfer Learning

- [ ] ！！ ~~U-BERT: Pre-Training User Representations for Improved Recommendation~~

- [x] 【UED】 **A Unified Pretraining Framework for Passage Ranking and Expansion**

  > we propose a general pretraining framework to enhance both the retrieval and re-ranking stages with a uniﬁed encoder-decoder network.

- [ ] ~~！！**Towards Semantics-Enhanced Pre-Training: Can Lexicon Definitions Help Learning Sentence Meanings~~**

可解释性：

- [ ] The Heads Hypothesis: A Unifying Statistical Approach towards Understanding Multi-Headed Attention in BERT

- [x] Self-Attention Attribution: Interpreting Information Interactions Inside Transformer

  > use the attribution scores to derive the interaction graphs, which visualizes the information ﬂow of Transformer. Moreover, we use the proposed method to identify the most important attention heads, which leads to a new head pruning algorithm. Finally, we show that ATTATTR can also be employed to construct adversarial triggers to implement non-targeted attacks.

表示学习：

- [ ] LRSC: Learning Representations for Subspace Clustering
- [ ] Uncertainty-Aware Multi-View Representation Learning
- [ ] Multi-View information-Bottleneck Representation Learning

其他：

- [ ] Robust Model Compression Using Deep Hypotheses
- [ ] Evidence Inference Networks for Interpretable Claim Verification
- [ ] Retrospective Reader for Machine Reading Comprehension
- [ ] Natural Language Inference in Context - Investigating Contextual Reasoning over Long Texts
- [ ] Distributed Ranking with Communications: Approximation Analysis and Applications
- [ ] Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders
- [ ] 【matching】Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-Based Dialogues
- [ ] 【matching】Making the Relation Matters: Relation of Relation Learning Network for Sentence Semantic Matching
- [ ] 【matching】Graph-Based Tri-Attention Network for Answer Ranking in CQA
- [ ]  【matching】A Few Queries Go a Long Way: Information-Distortion Tradeoffs in Matching
- [ ] 【matching】 A Graph-Based Relevance Matching Model for Ad-Hoc Retrieval
- [ ] Semantics-Aware Inferential Network for Natural Language Understanding



# NAACL2018

- [x] Contextualized word representations for reading comprehension.（NAACL2018）

# NAACL2019





# CIKM2018

- [x] From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing
- [x] Retrieve-and-read: Multi-task learning of information retrieval and reading comprehension.
- [ ] 



# CIKM2019

- [ ] （表示学习）Beyond word2vec: Distance-graph Tensor Factorization for Word and Document Embeddings
- [ ] （dialogue，response选择，捕捉多粒度信息）Multi-Turn Response Selection in Retrieval-Based Chatbots with Iterated Attentive Convolution Matching Network
- [ ] （对话）A Hybrid Retrieval-Generation Neural Conversation Model
- [ ] （对话）Attentive History Selection for Conversational Question Answering
- [ ] （知识图谱QA、用户反馈）An Interactive Mechanism to Improve Question Answering Systems via Feedback
- [ ] （构建适合语义检索的稠密向量搜索引擎）GRIP: Multi-Store Capacity-Optimized High-Performance Nearest Neighbor Search for Vector Search Engine （不用看）

------ 短文

- [ ] ~~Machine Reading Comprehension: Matching and Orders~~
- [ ] ~~Cluster-Based Focused Retrieval~~
- [ ] （dialogue，response选择）Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots
- [ ] （learn2rank的训练策略、鲁棒性）Analysis of Adaptive Training for Learning to Rank in Information Retrieval



# ！CIKM2020

- [x] 【SMITH】（长文本匹配）Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching

  > 两层次的 dual Transformer架构，第一层建模句子块内文本，第二层由句子块建模文本表达。
  >
  > 提出预训练+微调的方法，其中预训练任务在BERT MLM的基础上加入sentence block mask language model。

- [x] Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems

- [x] Dual Head-wise Coattention Network for Machine Comprehension with Multiple-Choice Questions

- [ ] Robust Retrievability based Document Selection for Relevance Feedback with Automatically Generated Query Variants（找不到）

- [ ] Transformer Model Compression via Joint Structured Pruning and Knowledge Distillation（找不到）

- [ ] TABLE: A Task-Adaptive BERT-based ListwisE Ranking Model for Document Retrieval（找不到）

- [x] CGTR: Convolution Graph Topology Representation for Document Ranking

  > we propose a GCN based method to encode global structure in contextual embeddings to improve ad-hoc document retrieval.

- [x] 【DMIN】Deep Multi-Interest Network for Click-through Rate Prediction

  > Distant Supervision in BERT-based Adhoc Document Retrieval
  >
  > Multi-Interest Network with Dynamic Routing for Recommendation at Tmall

- [x] Distant supervision in BERT-based Ad-hoc Document Retrieval

  > We find that direct transfer of relevance labels from documents to passage introduces label noise and it can not leverage information from large amount of unlabelled documents.
  >
  > We show that our distantly supervised models(Qa-DocRank and QA-Full-DOCRANK) perform better than the baselines. Extracting relevant passages from the unlabelled documents (QA-Full-DOCRANK) helps in solving the scarcity problem.
  >
  > Qa-DocRank：先用一个QA_model对passage进行打分，根据设定阈值得到query-passage的相关性label，再用这些labeled pair微调BERT模型。推断时是先得到每个passage的得分，再根据FirstP/MaxP/SumP/AvgP等策略得到最终得分。
  >
  > QA-Full-DOCRANK：在Qa-DocRank的基础上，用一个QA_model对无标签的document的passage进行打分，根据设定阈值得到query-passage的相关性label，再用这些labeled pair微调BERT模型。

# CIKM2021

- [x] Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance

  > 联合优化dense retriever和OPQ索引：1）ranking-oriented loss, 2）PQ centroid optimization, 3）end-to-end negative sampling。

- [ ] 



# ！WSDM2018（共录取84篇，录取率16%）

- [x] （匹配，迁移学习）Modelling Domain Relationships for Transfer Learning on Chatbot-based Question Answering Systems
- [x] ！！（ConvKNRM）Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search
- [x] （HyperQA）Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering
- [ ] （XXX）Query Driven Algorithm Selection in Early Stage Retrieval
- [x] ！！（Co-PACRR）ERM-PACRR: A Neural IR model with Enhanced Relevance Matching
- [x] Neural Ranking Models with Multiple Document Fields



# ！WSDM2019

- [ ] （xxxx,ranking）WassRank: Listwise Document Ranking Using Optimal Transport Theory
- [ ] （xxxx,ranking）Joint Optimization of Cascade Ranking Models
- [x] （匹配，迁移学习，强化学习）Learning to Selectively Transfer: Reinforced Transfer Learning for Deep Text Matching
- [x] （QA）Learning to Transform, Combine, and Reason in Open-Domain Question Answering
- [x] （对话）Multi-Representation Fusion Network for Multi-Turn Response Selection in Retrieval-Based Chatbots
- [ ] （XXX,表达学习）SHNE: Representation Learning for Semantic-Associated Heterogeneous Networks.
- [x] （电商）Weakly Supervised Co-Training Query Rewriting and Semantic Matching for E-Commerce
- [x] （推荐）Gated Attentive-Autoencoder for Content-Aware Recommendation
- [x] （推荐）Product-Aware Answer Generation in E-Commerce Question-Answering



# ！WSDM2020

- [x] 【TMKD】Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System

  > we propose a novel Two-stage Multi-teacher Knowledge Distillation approach for model compression:1) a Q&A multi-teacher distillation task is proposed for student model pre-training,2) a multi-teacher paradigm is designed to jointly learn from multiple teacher models (m-o-1) for more generalized knowledge distillation on downstream specific tasks.



# WWW2019

- [ ] （IR）Adversarial Sampling and Training for Semi-Supervised Information Retrieval
- [ ] What We Vote for? Answer Selection from User Expertise View in Community Question Answering
- [ ] （匹配模型）Semantic Text Matching for Long-Form Documents
- [ ] （表示学习）Learning Graph Pooling and Hybrid Convolutional Operations for Text Representations
- [ ] （问答）HAR: A Hierarchical Attention Retrieval Model for Healthcare Question Answering
- [ ] （匹配模型）Learning Fast Matching Models from Weak Annotations
- [ ] （表示学习）Semantic Hilbert Space for Text Representation Learning
- [ ] （问答）Focusing Attention Network for Answer Ranking
- [ ] （问题生成）Inferring Search Queries from Web Documents via a Graph-Augmented Sequence to Attention Network



# WWW2020

- [x] Leveraging Passage-level Cumulative Gain for Document Ranking

  > 找人标一批PCG数据，然后提出一个模型预测一个doc的PCG序列，进而就得到了doc的全局得分。we proposed a BERT-based sequential model PCGM for modeling PCG, which uses an LSTM after a pre-trained BERT with a gain embedding layer and a gain mask mechanism.

- [x] Selective Weak Supervision for Neural Information Retrieval

  > ReInfoSelect leverages the widely available anchor data to weakly supervise neural rankers.
  >
  > To handle the noises in anchor data, ReInfoSelect uses policy gradient to connect the demand—the needs of training signals from neural ranker—and the supplyanchor-document pairs, to select more effective weak supervision signals.



# WWW2021

- [x] Generalizing Discriminative Retrieval Models using Generative Tasks

  > 用生成任务作为辅助任务训练判别检索模型，不仅可以使得检索性能更好，还能提高模型的迁移性。

- [ ] 

  



# SIGKDD

# ICDM

# COLING

# CoNLL







# 模型压缩

##### 剪枝

- [ ] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned.（2019）
- [ ] Are sixteen heads really better than one?（NIPS2019）

##### Quantization 方法

- [ ] Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.（ICLR2015）
- [ ] Deep learning with limited numerical precision.（ICML2015）
- [ ] Training and inference with integers in deep neural networks.（ICLR2018）
- [ ] Efficient Weights Quantization of Convolutional Neural Networks Using Kernel Density Estimation based Non-uniform Quantizer.（2019）

##### binarized networks

- [ ] Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1.（2016）

##### 蒸馏

- [ ] 【知识蒸馏】Model compression（2006，Bucilua，首次提出通过知识蒸馏压缩模型的思想，但是没有实际的工作阐述）
- [ ] 【知识蒸馏】Do deep nets really need to be deep?（2014）
- [ ] 【知识蒸馏】Distilling the knowledge in a neural network.（2015，Hinton，第一次正式定义Distillation，并提出相应的训练方法）

--------------------------------------------------

- [ ] 【综述】Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks.（2020）
- [ ] 【综述】Knowledge Distillation: A Survey. （2020）

---------------------

- [x] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.（NIPS2019，将BERT-base通过预训练阶段蒸馏到6层。loss=MLM + 首层embedding的cosine embedding loss + 预测层输出类别分布蒸馏loss。实验是GLUE、SQuAD）

- [x] TinyBERT: Distilling BERT for Natural Language Understanding.（2019，将BERT-base通过预训练阶段蒸馏到少层。loss=embedding层蒸馏 + trm attention matrix蒸馏loss + trm hidden state蒸馏loss + 预测层输出类别分布蒸馏loss。实验是GLUE）

- [ ] Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers（2020）

- [ ] Well-read students learn better: The impact of student initialization on knowledge distillation.（2019）

- [x] 【蒸馏成dual encoder】**Distilling Task-Specific Knowledge from BERT into Simple Neural Networks**.（2019，distilling the **BERT** model into a two-tower model with **one-layer BiLSTM** as encoders。loss=预测层输出蒸馏loss + ground truth loss。使用数据增强得到无标签数据集进一步帮助知识蒸馏。实验是SST-2、QQP、MNLI）

- [x] 【蒸馏成dual encoder】**Scalable attentive sentence-pair modeling via distilled sentence embedding**.（AAAI2020，Given a cross-attentive teacher model (e.g. a fine-tuned **BERT-large**), we train a sentence embedding based student model（**BERT-large**） to reconstruct the sentence-pair scores obtained by the teacher model。loss=预测层输出蒸馏loss + ground truth loss。实验是GLUE中的5个sentence-pair任务）

- [x] 【TMKD】Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System（WSDM2020，预训练蒸馏+微调蒸馏，m-to-1模式，student模型是BiLSTM或者BERT。实验是QA、MNLI、SNLI、RTE）  

- [x] 【蒸馏成dual encoder】**Distilling Knowledge for fast retrieval-based chat-bots**（SIGIR2020。student model是BiLSTM或者BERT。loss=预测层输出蒸馏loss + ground truth loss。）

- [x] 【蒸馏成dual encoder】**DiPair: Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling**（2020）

- [x] Understanding bert rankers under distillation.

  > 在MSMARCO上由bert reranker蒸馏一个低层的bert reranker。提出LM distilled和Ranker Distilled。

- [ ] Distilling Knowledge Learned in BERT for Text Generation（ACL2020）

- [ ] TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing（ACL2020）

- [ ] DE-RRD: A Knowledge Distillation Framework for Recommender System

- [x] DistilSum: Distilling the Knowledge for Extractive Summarization

- [ ] Distilling Structured Knowledge into Embeddings for Explainable and Accurate Recommendation（

- [ ] ~~Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation~~

  > 作者提出了基于知识蒸馏的多语言sentence embedding训练方法，该方法可以**将已有的单语言模型扩展为多语言模型**。

- [x] *Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data*

  > Our method involves two phases of distillation. The ﬁrst phase is at the data level as it constructs (i.e., distills) postresponse pairs by matching sentences retrieved from a set of unpaired data. The second phase is at the model-level as it distills a teacher model using the augmented data.

- [ ] *Lifelong Language Knowledge Distillation*

- [ ] Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers. 

  > we proposed a novel model to distill from intermediate layers as well as ﬁnal predictions.
  
- [ ] ~~Contrastive Representation Distillation~~

- [ ] Patient knowledge distillation for bert model compression（sun, 层与层的蒸馏）

  > <img src="../images/image-20201221112254679.png" alt="image-20201221112254679" style="zoom:50%;" />
  >
  > <img src="../images/image-20201221112327072.png" alt="image-20201221112327072" style="zoom:33%;" />
  >
  > <img src="../images/image-20201221112349177.png" alt="image-20201221112349177" style="zoom:33%;" />
  >
  > <img src="../images/image-20201221112405716.png" alt="image-20201221112405716" style="zoom:33%;" />



# 长本文

- [x] ETC: Encoding Long and Structured Inputs in Transformers

- [ ] Hibert: Document level pre-training of hierarchical bidirectional transformers for document summarization.

- [ ] Generating long sequences with sparse transformers.

- [ ] Big bird: Transformers for longer sequences.

  

处理长文本的一般方式：

1. sliding window

- [x] Multi-passage bert: A globally normalized bert model for open-domain question answering.

2. 改进Transformer结构

- [ ] Reformer: The efﬁcient transformer.
- [ ] Compressive transformers for long-range sequence modelling.
- [ ] Transformer-xl: Attentive language models beyond a ﬁxed-length context.
- [ ] Adaptive attention span in transformers.
- [ ] Longformer: The long-document transformer.

3. 压缩输入文本

- [x] CogLTX: Applying BERT to Long Texts

- [ ] Text summarization with pretrained encoders.（文本摘要的SOTA方法）

  

# bias 问题

- [x] Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems（推荐系统）

- [x] Debiased Contrastive Learning

  

- [x] Learning to rank with **selection bias** in **personal search**（2016，提出Inverse propensity weighting，使用result randomization进行examination propensity）

- [x] Unbiased learning-to-rank with biased feedback.（2017，提出Inverse propensity weighting，使用swap-intervention进行examination propensity）
- [x] Unbiased Learning to Rank with Unbiased Propensity Estimation（2018，艾，郭，使用对偶学习算法进行examination propensity）
- [x] Position Bias Estimation for Unbiased Learning to Rank in Personal Search.（2018，提出Regression EM）
- [ ] Unbiased LambdaMART: An Unbiased Pairwise Learning-to-Rank Algorithm（2019，a variation of DLA with pairwise ranking losses）
- [x] Unbiased Learning to Rank: Online or Offline?（2020，艾）





























